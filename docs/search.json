[
  {
    "objectID": "hw5.html",
    "href": "hw5.html",
    "title": "Homework 5",
    "section": "",
    "text": "Using cross validation helps verify that the random forest model actually fits the data in a meaningful way. It’s possible to create a random forest model which fits the test data set very well, but isn’t particularly helpful when it comes to the population as a whole. Cross validation which catch such issues.\nThe bagged tree algorithm helps to reduce the variance of the decision tree estimators by averaging the prediction of some B number of trees, from B bootstrapped data sets for training.\nA general linear model (or glm) in R is simply any model which relies somewhat on the basis of linear regression, even if the model itself is non-linear.\nThe interaction term allows the model to determine the effect of the presence of two predictors at once which it would otherwise be unable to detect from the presence of the two predictors separately. It also is then able to return a better interpretation of the effect of each individual predictor than otherwise.\nSplitting a data set into test and training data sets is for model validation. Some model types, such as decision tree and random forest models, will sometimes over fit a data set, so the test data set ensures that the model still performs well even on data from the same population that it was not trained on."
  },
  {
    "objectID": "hw5.html#task-1-conceptual-questions",
    "href": "hw5.html#task-1-conceptual-questions",
    "title": "Homework 5",
    "section": "",
    "text": "Using cross validation helps verify that the random forest model actually fits the data in a meaningful way. It’s possible to create a random forest model which fits the test data set very well, but isn’t particularly helpful when it comes to the population as a whole. Cross validation which catch such issues.\nThe bagged tree algorithm helps to reduce the variance of the decision tree estimators by averaging the prediction of some B number of trees, from B bootstrapped data sets for training.\nA general linear model (or glm) in R is simply any model which relies somewhat on the basis of linear regression, even if the model itself is non-linear.\nThe interaction term allows the model to determine the effect of the presence of two predictors at once which it would otherwise be unable to detect from the presence of the two predictors separately. It also is then able to return a better interpretation of the effect of each individual predictor than otherwise.\nSplitting a data set into test and training data sets is for model validation. Some model types, such as decision tree and random forest models, will sometimes over fit a data set, so the test data set ensures that the model still performs well even on data from the same population that it was not trained on."
  },
  {
    "objectID": "hw5.html#task-2-fitting-models",
    "href": "hw5.html#task-2-fitting-models",
    "title": "Homework 5",
    "section": "Task 2: Fitting Models",
    "text": "Task 2: Fitting Models\nLet’s go ahead and get out data\n\nheart&lt;- read.csv(url(\"https://www4.stat.ncsu.edu/~online/datasets/heart.csv\"))\nhead(heart)\n\n  Age Sex ChestPainType RestingBP Cholesterol FastingBS RestingECG MaxHR\n1  40   M           ATA       140         289         0     Normal   172\n2  49   F           NAP       160         180         0     Normal   156\n3  37   M           ATA       130         283         0         ST    98\n4  48   F           ASY       138         214         0     Normal   108\n5  54   M           NAP       150         195         0     Normal   122\n6  39   M           NAP       120         339         0     Normal   170\n  ExerciseAngina Oldpeak ST_Slope HeartDisease\n1              N     0.0       Up            0\n2              N     1.0     Flat            1\n3              N     0.0       Up            0\n4              Y     1.5     Flat            1\n5              N     0.0       Up            0\n6              N     0.0       Up            0\n\n\n\nQuick EDA/ Data Preparation\n\nWe’re going to check on missing data and summarize the data, especially with respect to the relationships of variables to the response: HeartDisease\n\n#Do we have any missing response? \nsum(is.na(heart$HeartDisease)==T) #No. \n\n[1] 0\n\n#Is anything missing at all? \nsum(is.na(heart)==T) #No. \n\n[1] 0\n\n#Let's look at heart disease by age and sex\nlibrary(ggplot2)\nggplot(data = heart, aes(y = as.factor(HeartDisease), color = as.factor(Age))) +geom_bar()\n\n\n\n\n\n\n\nggplot(data = heart, aes(y = as.factor(HeartDisease), color = as.factor(Sex))) +geom_bar()\n\n\n\n\n\n\n\n\nNeed to change the HeartDisease variable to a factor and remove the ST_Slope variable.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nheart&lt;- heart %&gt;% mutate(HeartDisease = as.factor(HeartDisease)) %&gt;% \n  select(-ST_Slope)\n\nIn preparation for kNN, we’re going to use dummy numeric variables for Sex, Chest Pain Type, Exercise Angina, and Resting ECG.\n\nlibrary(caret) #We'll need this library for it.\n\nWarning: package 'caret' was built under R version 4.3.3\n\n\nLoading required package: lattice\n\n#I'm going to do it this way\nheart&lt;- heart %&gt;% mutate(Sex = as.factor(Sex)) %&gt;%\n  mutate(ChestPainType = as.factor(ChestPainType)) %&gt;%\n  mutate(ExerciseAngina = as.factor(ExerciseAngina)) %&gt;%\n  mutate(RestingECG = as.factor(RestingECG))\n\n\n\n\nkNN\n\n#First, we need to split the data into pieces, for test and train. \nset.seed(1701) #To boldy go where no seed has gone before\nfortrain&lt;- createDataPartition(y = heart$HeartDisease, p = 0.66, list = F)\nhtrain&lt;- heart[fortrain,]\nhtest&lt;- heart[-fortrain,]\n\n#Now we get to specify the CV parameters. 10 folds. 3 repeats. \ncontrol&lt;- trainControl(method = \"repeatedCV\", number = 10, repeats = 3)\n\nWarning: `repeats` has no meaning for this resampling method.\n\n#And now we train the model. \nset.seed(1701) #A garden can never have too many seeds. Unless it's mint. \nheart_knn&lt;- train(HeartDisease~., data = htrain, method = \"knn\",\n                            trControl = control,\n                            preProcess = c(\"center\", \"scale\"),\n                            tuneLength = 40)\n\n#Now we get to see how well it did\nhtest_pred&lt;- predict(heart_knn, newdata = htest)\nconfusionMatrix(htest_pred, htest$HeartDisease)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 114  34\n         1  25 138\n                                          \n               Accuracy : 0.8103          \n                 95% CI : (0.7622, 0.8523)\n    No Information Rate : 0.5531          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.6186          \n                                          \n Mcnemar's Test P-Value : 0.2976          \n                                          \n            Sensitivity : 0.8201          \n            Specificity : 0.8023          \n         Pos Pred Value : 0.7703          \n         Neg Pred Value : 0.8466          \n             Prevalence : 0.4469          \n         Detection Rate : 0.3666          \n   Detection Prevalence : 0.4759          \n      Balanced Accuracy : 0.8112          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\n\nLogistic Regression\nLet’s see if we can create a better fitting GLM model\n\n#So glm doesn't have a neat way to do CV so here goes\n#Randomly shuffle the data\nset.seed(1701)\nheart&lt;-heart[sample(nrow(heart)),]\n\n#Create 10 equally size folds\nfolds &lt;- cut(seq(1,nrow(heart)),breaks=10,labels=FALSE)\n\n#Perform 10 fold cross validation\nfor(i in 1:10){\n    #Segement data by fold using the which() function \n    testIndexes &lt;- which(folds==i,arr.ind=TRUE)\n    htrain_glm &lt;- heart[testIndexes, ]\n    htest_glm &lt;- heart[-testIndexes, ]\n}\n\n#Going for a binomial family to start with since heart disease is binomial\nset.seed(1701)\nheart_glm1&lt;- glm(HeartDisease~., data = htrain_glm, family = \"binomial\")\nsummary(heart_glm1)\n\n\nCall:\nglm(formula = HeartDisease ~ ., family = \"binomial\", data = htrain_glm)\n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)       0.488341   4.051327   0.121   0.9041  \nAge              -0.031921   0.041448  -0.770   0.4412  \nSexM              1.898088   0.898906   2.112   0.0347 *\nChestPainTypeATA -3.286764   1.345513  -2.443   0.0146 *\nChestPainTypeNAP -0.443010   0.836212  -0.530   0.5963  \nChestPainTypeTA  -2.578496   1.435595  -1.796   0.0725 .\nRestingBP         0.001207   0.019261   0.063   0.9500  \nCholesterol      -0.002127   0.003152  -0.675   0.4997  \nFastingBS         0.351000   0.806088   0.435   0.6632  \nRestingECGNormal  0.203950   0.839286   0.243   0.8080  \nRestingECGST      2.483028   1.165878   2.130   0.0332 *\nMaxHR            -0.006953   0.012645  -0.550   0.5824  \nExerciseAnginaY   0.245931   0.744902   0.330   0.7413  \nOldpeak           1.111426   0.444220   2.502   0.0124 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 126.84  on 91  degrees of freedom\nResidual deviance:  68.78  on 78  degrees of freedom\nAIC: 96.78\n\nNumber of Fisher Scoring iterations: 6\n\n#This should be terrible\nheart_glm2&lt;- glm(HeartDisease~log(Age)+Sex+Sex*log(Age), data = htrain_glm, family = \"binomial\")\nsummary(heart_glm2) #Not as bad as I thought though\n\n\nCall:\nglm(formula = HeartDisease ~ log(Age) + Sex + Sex * log(Age), \n    family = \"binomial\", data = htrain_glm)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)    -0.7415    13.3943  -0.055    0.956\nlog(Age)       -0.2798     3.3927  -0.082    0.934\nSexM           -6.6759    14.3650  -0.465    0.642\nlog(Age):SexM   2.1958     3.6328   0.604    0.546\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 126.84  on 91  degrees of freedom\nResidual deviance: 111.35  on 88  degrees of freedom\nAIC: 119.35\n\nNumber of Fisher Scoring iterations: 4\n\n#Let's try this\nheart_glm3&lt;- glm(HeartDisease~log(Age)+Sex+log(Cholesterol+1)+log(Cholesterol+1)*log(Age), data = htrain_glm, family = \"binomial\")\n\nsummary(heart_glm3)\n\n\nCall:\nglm(formula = HeartDisease ~ log(Age) + Sex + log(Cholesterol + \n    1) + log(Cholesterol + 1) * log(Age), family = \"binomial\", \n    data = htrain_glm)\n\nCoefficients:\n                              Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)                   -11.5048    21.6204  -0.532  0.59464   \nlog(Age)                        2.6859     5.3523   0.502  0.61579   \nSexM                            1.8436     0.6797   2.712  0.00668 **\nlog(Cholesterol + 1)            0.7995     3.9993   0.200  0.84156   \nlog(Age):log(Cholesterol + 1)  -0.2463     0.9905  -0.249  0.80364   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 126.84  on 91  degrees of freedom\nResidual deviance: 109.11  on 87  degrees of freedom\nAIC: 119.11\n\nNumber of Fisher Scoring iterations: 4\n\n#Okay, the first one did best based on AIC so\n\nheart_glm_predict&lt;- predict.glm(heart_glm1, newdata = htest_glm) #This gives log odds and I need 0s and 1\n\nheart_glm_pred_prob&lt;- exp(heart_glm_predict)/(1+exp(heart_glm_predict))\n\nconfusionMatrix(as.factor(round(heart_glm_pred_prob)), as.factor(htest_glm$HeartDisease)) #Close but not the winner\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 289 111\n         1  71 355\n                                          \n               Accuracy : 0.7797          \n                 95% CI : (0.7498, 0.8075)\n    No Information Rate : 0.5642          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.5575          \n                                          \n Mcnemar's Test P-Value : 0.003842        \n                                          \n            Sensitivity : 0.8028          \n            Specificity : 0.7618          \n         Pos Pred Value : 0.7225          \n         Neg Pred Value : 0.8333          \n             Prevalence : 0.4358          \n         Detection Rate : 0.3499          \n   Detection Prevalence : 0.4843          \n      Balanced Accuracy : 0.7823          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\n\nTree Models (Ents with Stats degrees???)\nWe’re going to use repeated 10 fold CV to get a classification tree model, random forest, and boosted tree.\n\nClassification tree model\n\nheart_classtree&lt;- train(HeartDisease~., data = htrain, method = \"rpart\",\n                            trControl = control,\n                            preProcess = c(\"center\", \"scale\"),\n                            cp = 0.1)\nheart_ctpred&lt;- predict(heart_classtree, htest)\nconfusionMatrix(heart_ctpred, htest$HeartDisease) #KNN still winning so far\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 115  37\n         1  24 135\n\n               Accuracy : 0.8039          \n                 95% CI : (0.7553, 0.8465)\n    No Information Rate : 0.5531          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n\n                  Kappa : 0.6068          \n\n Mcnemar's Test P-Value : 0.1244          \n\n            Sensitivity : 0.8273          \n            Specificity : 0.7849          \n         Pos Pred Value : 0.7566          \n         Neg Pred Value : 0.8491          \n             Prevalence : 0.4469          \n         Detection Rate : 0.3698          \n   Detection Prevalence : 0.4887          \n      Balanced Accuracy : 0.8061          \n\n       'Positive' Class : 0               \n\n\n\nRandom Forest (If the forest wasn’t there yesterday, it’s either Ents or Duncan coming for MacBeth)\n\nmtry&lt;- sqrt(ncol(htrain))\ntunegrid&lt;- expand.grid(.mtry = mtry)\n\nheart_rf&lt;- train(HeartDisease~., data = htrain, method = \"rf\",\n                            trControl = control,\n                            preProcess = c(\"center\", \"scale\"),\n                            tuneGrid = tunegrid)\nheart_rf_pred&lt;- predict(heart_rf, htest)\nconfusionMatrix(heart_rf_pred,htest$HeartDisease) #Booo it's the best so far\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 109  24\n         1  30 148\n\n               Accuracy : 0.8264          \n                 95% CI : (0.7796, 0.8668)\n    No Information Rate : 0.5531          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n\n                  Kappa : 0.6473          \n\n Mcnemar's Test P-Value : 0.4962          \n\n            Sensitivity : 0.7842          \n            Specificity : 0.8605          \n         Pos Pred Value : 0.8195          \n         Neg Pred Value : 0.8315          \n             Prevalence : 0.4469          \n         Detection Rate : 0.3505          \n   Detection Prevalence : 0.4277          \n      Balanced Accuracy : 0.8223          \n\n       'Positive' Class : 0               \n\n\n\nBoosted tree\n\nlibrary(gbm)\n\nWarning: package 'gbm' was built under R version 4.3.3\n\n\nLoaded gbm 2.2.2\n\n\nThis version of gbm is no longer under development. Consider transitioning to gbm3, https://github.com/gbm-developers/gbm3\n\ntunegrid&lt;- expand.grid(n.trees = c(25,50,100,200),\n                       interaction.depth = c(1,2,3),\n                       shrinkage = 0.1,\n                       n.minobsinnode = 10)\n\nset.seed(1701)\nheart_bt&lt;-  train(HeartDisease~., data = htrain, method = \"gbm\",\n                            trControl = control,\n                            preProcess = c(\"center\", \"scale\"),\n                            tuneGrid = tunegrid, \n                            verbose = F)\n\nheart_bt_pred&lt;- predict(heart_bt, newdata = htest)\nconfusionMatrix(heart_bt_pred, htest$HeartDisease) #WINNER!!!\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 118  29\n         1  21 143\n\n               Accuracy : 0.8392          \n                 95% CI : (0.7936, 0.8783)\n    No Information Rate : 0.5531          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n\n                  Kappa : 0.6766          \n\n Mcnemar's Test P-Value : 0.3222          \n\n            Sensitivity : 0.8489          \n            Specificity : 0.8314          \n         Pos Pred Value : 0.8027          \n         Neg Pred Value : 0.8720          \n             Prevalence : 0.4469          \n         Detection Rate : 0.3794          \n   Detection Prevalence : 0.4727          \n      Balanced Accuracy : 0.8402          \n\n       'Positive' Class : 0               \n\n\n\n\n\n\nWrap Up\nWith an accuracy of 83.9%, the boosted tree did the best job!!!"
  }
]